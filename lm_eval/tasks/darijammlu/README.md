# DarijaMMLU

<!-- ### Paper

Title: N/A

Abstract: N/A

N/A

Homepage: N/A


### Citation

```
@article{Not yet published!
}
``` -->

### Groups, Tags

#### Groups

* `darijammlu`: evaluates all DarijaMMLU tasks.

#### Tags
Source-based tags:

* `darijammlu_mmlu`: evaluates DarijaMMLU tasks that were translated from MMLU.
* `darijammlu_ar_mmlu`: evaluates DarijaMMLU tasks that were translated from ArabicMMLU.

Category-based tags:

* `darijammlu_stem`: evaluates DarijaMMLU STEM tasks.
* `darijammlu_social_sciences`: evaluates DarijaMMLU social sciences tasks.
* `darijammlu_humanities`: evaluates DarijaMMLU humanities tasks.
* `darijammlu_language`: evaluates DarijaMMLU language tasks.
* `darijammlu_other`: evaluates other DarijaMMLU tasks.

### Checklist

For adding novel benchmarks/datasets to the library:
* [ ] Is the task an existing benchmark in the literature?
  * [ ] Have you referenced the original paper that introduced the task?
  * [ ] If yes, does the original paper provide a reference implementation? If so, have you checked against the reference implementation and documented how to run such a test?


If other tasks on this dataset are already supported:
* [ ] Is the "Main" variant of this task clearly denoted?
* [ ] Have you provided a short sentence in a README on what each new variant adds / evaluates?
* [ ] Have you noted which, if any, published evaluation setups are matched by this variant?
